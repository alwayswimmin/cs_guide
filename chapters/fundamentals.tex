\chapter{Fundamentals}

\section{Input and Output}

The first part of solving any programming contest problem is reading the input correctly. In this section, we'll briefly go over input and output in Java using \texttt{java.util.Scanner} and \texttt{java.io.PrintWriter}. There are two scenarios that you should be familiar with: \texttt{stdin}/\texttt{stdout} and file I/O. You encounter the former when you enter input and see output for a program run in the commandline. You encounter the latter when you have two files (for example \texttt{input.txt} and \texttt{output.txt}) that you read from and write to.

When using \texttt{stdin}/\texttt{stdout}, we read input from \texttt{System.in} using a Scanner and output our results using \texttt{System.out.println()}. \texttt{Scanner.nextInt()} and \texttt{Scanner.next()} read in integers and strings, respectively. \texttt{System.out.println()} prints its argument and adds a newline at the end. (If we don't want the newline, we can use \texttt{System.out.print()}.) Here's an example of a main method that takes two integers and outputs their sum:

\begin{mylstlisting}
public static void main(String args[]){
  // hint: you should write "import java.util.*;" at the top of your code.
  Scanner sc = new Scanner(System.in);
  int x = sc.nextInt();
  int y = sc.nextInt();
  System.out.println(x + y);
}
\end{mylstlisting}

File I/O is a touch more complicated. For our Scanner, we have to make a new File object and use it in the constructor. We do the same for our PrintWriter. However, PrintWriter also comes with a couple more usage notes. First, we should include \texttt{throws IOException} after our main method, since Java requires that we acknowledge the possibility of an IOException. (We bravely assume that our file open will succeed!) After we finish printing, we must also close the PrintWriter to make sure that everything gets written. Here's a snippet showing how Scanner and PrintWriter work with files: 

\begin{mylstlisting}
public static void main(String args[]) throws IOException {
  // hint: for file I/O, you should also have "import java.io.*;"
  Scanner sc = new Scanner(new File("input.txt"));
  int x = sc.nextInt();
  int y = sc.nextInt();
  PrintWriter pw = new PrintWriter(new File("output.txt"));
  pw.println(x + y);
  pw.close();
}
\end{mylstlisting}

Although more efficient methods of I/O exist, such as BufferedReader and BufferedWriter, what's covered here should be sufficient for now. (It's possible to read in $10^5$ integers with Scanner in a fraction of a second.)

\section{Complexity}

Before we start computing contests, we should understand what the problems on these contests are trying to test. One class of problems, called \emph{implementation problems}, assesses your ability write code quickly and accurately. These problems are only common in easier contests, since they usually don't involve too much thinking or creativity---you just have to implement what's written in the problem statement. Most competitive programming problems ask you to come up with a clever algorithm instead, testing speed and memory efficiency.

To analyze the efficiency of algorithms, computer scientists use a concept called \emph{complexity}. Complexity is measured as a function of the input size; an algorithm could require $3n$, $n^4/3$ or even $2^n + n^2$ steps to finish calculating for an input of size $n$. To express complexity, we use something called ``big-O notation.'' Essentially, we write the number of steps it takes for an algorithm to finish inside a pair of parentheses with an $O$ in front, like this: $O(\text{\# of steps})$. However, we drop any constant factors and lower order terms from the expression.\footnote{Unfortunately, this isn't entirely accurate. Saying an algorithm is $O(f(n))$ actually means it takes \emph{at most} $c\cdot f(n)$ steps to finish, for a sufficiently large constant $c$.} I'll explain why in a moment; let's look at some examples for now. 

Suppose we have three programs that require $3n$, $n^4/3$ and $2^n + n^2$ steps to finish, respectively. The complexity of the first program is $O(n)$ because we don't care about the constant factor $3$ on the $3n$. The complexity of the second program is $O(n^4)$; again, we drop the constant. For the last program, we write its complexity as $O(2^n)$ because $n^2$ is a lower order term.

As to why we drop the constants and the lower order terms, consider the first two programs from above. When $n=300$, the first program takes $900$ steps, while the second program takes $2,700,000,000$ steps. The second program is much slower, despite a smaller constant factor. Meanwhile, if we had a third program that runs in $5n$ steps, it would still only take $1,500$ steps to finish. Constant factors become pretty much irrelevant when we're comparing functions that grow at different rates. The same can be said about lower order terms: $n^2$ gets dwarfed by $2^n$ even when $n=10$.

Thus in programming contests, we usually want a program with the correct complexity, without worrying about too much constant factors. Complexity will be the difference between whether a program gets \textbf{\color{green}accepted} or \textbf{\color{red}time limit exceeded}. As a rule of thumb, a modern processor can do around $10^8$ computations each second. When you plug the maximum possible $n$ into the complexity of your algorithm, it should never be much more than that.

We've focused on time and haven't talked much about memory so far, but memory does gets tested. The amount of memory a program uses as a function of $n$ is called its \emph{space complexity}, as opposed to the \emph{time complexity} we discussed earlier. Space complexity however, shows up much less frequently than time complexity. One of the reasons is that you usually run out of time before you can exceed the memory limit.

\section{Sorting}

To further explore the concept of complexity, we will use sorting algorithms as a case study. Sorting is just as it sounds---we're given a collection of objects, and we want to sort them into a predefined order.

\subsection{Insertion Sort}

Insertion sort builds up a sorted list by inserting new elements one at a time. Inserting an element into a sorted list takes time proportional to the length of the list, so the runtime of this algorithm is $1 + 2 + 3 + \cdots + n$, which is $O(n^2)$. One way to think about this is to iterate $i$ from $1$ to $n$, and let the first $i$ elements be our sorted list. To insert the $(i+1)$th element, we just swap it with the largest, the second largest, and so on, until it's greater than than the next largest element. Then we have a sorted list in the first $(i+1)$ entries of the array. Insertion sort, despite being slower than merge sort and quicksort, is still useful because of its efficiency on small inputs. Many implementations of merge sort and quicksort actually use insertion sort once the problem size gets small.

Around how long is the longest list that you can sort with insertion sort in less than a second?

\subsection{Merge Sort}

The idea behind merge sort is the following observation: If we are given two sorted lists of length $n/2$, we only need $n$ comparisons to merge them into one sorted list of length $n$. So we can divide and conquer. We do this by cutting the array in half, sorting each half recursively, and merging the two halves back together. Because our recursion goes $\log_2 n$ levels deep, this algorithm runs in $O(n \log n)$. Try to work out the details of this yourself. (You can in fact prove that $O(n \log n)$ comparisons is optimal for sorting algorithms, one of the few problems in computer science that has a non-trivial lower bound.)

Around how long is the longest list that you can sort with merge sort in less than a second?

\subsection{Quicksort}

Quicksort also uses a divide and conquer strategy to run in $O(n \log n)$ on average. We first take a random element from the array, called the \emph{pivot}. Then, we move anything less than the pivot to the left of the array, anything greater than the pivot to the right of the array, and recurse on the two halves that we just created. We say that quicksort runs in $O(n \log n)$ only on average because there are cases that can make quicksort run in $O(n^2)$. What would happen if we chose the smallest element of the array as the pivot each time?

Quicksort was previously implemented by \texttt{Arrays.sort()} and \texttt{Collections.sort()}, but Java now uses dual-pivot quicksort and timsort.

\subsection{Comparison}

We'll finish our section on sorting by taking a look at how Java and C++ sorts objects. We also provide a brief glimpse into using standard data structures that require a natural ordering on the objects within the container.

In Java, there is a built-in sort method, \texttt{Arrays.sort()}, in \texttt{java.util.Arrays}, which takes an array and sorts it. To sort an array \texttt{int array[]}, we call \texttt{Arrays.sort(array)}. Java always sorts least to greatest.

If we want to sort an array of our own objects, we have to first implement the \texttt{Comparable} interface. This is how Java orders a set of objects. Implementing \texttt{Comparable} requires support of the method \texttt{int compareTo(Object o)}. If \texttt{a.compareTo(b) < 0}, then \texttt{a} comes before \texttt{b}; if \texttt{a.compareTo(b) > 0}, then \texttt{a} comes after \texttt{b}; if \texttt{a.compareTo(b) == 0}, then \texttt{a} is considered equal to \texttt{b}. (\texttt{a.compareTo(b) == 0} should be equivalent to \texttt{a.equals(b)}.) For example, consider the class \texttt{MyPair}:

\begin{mylstlisting}
class MyPair implements Comparable {
	int x;
	int y;
    // sort by x-coordinate first
	public int compareTo(Object o) {
		MyPair c = (MyPair) o;
		if(x < c.x) return -1;
		if(x > c.x) return 1;
		// if x-coordinates equal, compare y
		if(y < c.x) return -1;
		if(y > c.x) return 1;
		return 0; // equal
	}
}
\end{mylstlisting}

If you know generics (Section \ref{sec:generics}), we can clean this up a bit. Note that no casting is required anymore.

\begin{mylstlisting}
class MyPair implements Comparable<MyPair> {
	int x;
	int y;
	public int compareTo(MyPair c) {
		if(x < c.x) return -1;
		if(x > c.x) return 1;
		if(y < c.x) return -1;
		if(y > c.x) return 1;
		return 0;
	}
}
\end{mylstlisting}

We can sort much more than just arrays. Java provides us many data structures we can use, much more than just arrays. In this section, I'll mention briefly the \texttt{ArrayList} for Java and the \texttt{vector} for C++. If you don't know what either of these are, you can just pretend like they are arrays for now; they are very similar and will be covered in detail in a later chapter. To sort these more complex data structures in Java, we use \texttt{Collections.sort()}.

Suppose we wanted to code a custom comparison operation for something that already exists. For example, suppose we wanted to sort an array of \texttt{ArrayList}s based on the sum of the elements in the \texttt{ArrayList}. We clearly don't want to code an entire \texttt{ArrayList} just because we want to sort it in a special way. One way we can easily resolve this is by using \texttt{extends}.

\begin{mylstlisting}
class MyArrayList extends ArrayList<Integer> implements Comparable<MyArrayList> {
	public int compareTo(MyArrayList c) {
		int sum = 0;
		for(int k = 0; k < size(); k++) {
			sum += get(k);
		}
		int csum = 0;
		for(int k = 0; k < c.size(); k++) {
			csum += c.get(k);
		}
		if(sum < csum) return -1;
		if(sum == csum) return 0;
		return 1;
	}
}
\end{mylstlisting}

Alternatively, we could use a \texttt{Comparator}. This is considered much better practice than the hack I presented earlier by coding a new class with \texttt{extends}. \texttt{Comparator}s are especially useful if we are to compare the same object different ways in the same program. A \texttt{Comparator} is an entirely separate class that takes two objects and determines which ought to come before the other in the ordering. Here is the \texttt{Comparator} implementation of the example I showed above.

\begin{mylstlisting}
class Cmp implements Comparator<ArrayList<Integer>> {
	public int compare(ArrayList<Integer> o1, ArrayList<Integer> o2) {
		int sum1 = 0;
		for(int k = 0; k < o1.size(); k++) {
			sum1 += o1.get(k);
		}
		int sum2 = 0;
		for(int k = 0; k < o2.size(); k++) {
			sum2 += o2.get(k);
		}
		if(sum1 < sum2) return -1;
		if(sum1 == sum2) return 0;
		return 1;
	}
}
\end{mylstlisting}

Now, when we sort, we'll need to pass our \texttt{Comparator} in addition to the data structure to be sorted. For example, we use the code \texttt{Arrays.sort(array, new Cmp())}.

In C++, comparing objects is slightly different. We have three methods, and all can be used with \texttt{sort}, which is in \texttt{<algorithm>}, or with any other standard library function or data structure which requires an ordering. As in Java, sorting is done least to greatest.

The first method is to implement the method \texttt{operator<} in a \texttt{struct} or \texttt{class} we coded ourselves. Here's an example \texttt{struct my\_pair} which is equivalent to the above Java \texttt{class MyPair}.

\begin{mylstlisting}[language=C++]
struct my_pair {
	int x;
	int y;
	friend bool operator<(const my_pair& lhs, const my_pair& rhs) {
		return lhs.x < rhs.x || lhs.x == rhs.x && lhs.y < rhs.y;
    }
};
\end{mylstlisting}

Suppose we have an array \texttt{my\_pair array[]}. We can then call \texttt{sort(array, array + n)} to sort the first \texttt{n} elements of the array. If we have a different data structure, like \texttt{vector<my\_pair> v}, we can sort it using \texttt{sort(v.begin(), v.end())}.

The second method is to use a custom comparison function. This is very similar to what we would put in function contained in the Java \texttt{Comparator}, but is distinct from the \texttt{Comparator} itself as it is only a function and not a class. Suppose we wanted to sort pairs primarily increasing in the first element and secondarily decreasing in the second. It seems inefficient to code a new pair class when C++ provides one, so we can simply use a custom comparison function to get around this.

\begin{mylstlisting}[language=C++]
bool cmp(const std::pair<int, int>& lhs, const std::pair<int, int>& rhs) {
	return lhs.x < rhs.x || lhs.x == rhs.x && lhs.y > rhs.y;
}
\end{mylstlisting}

\texttt{cmp}, like \texttt{operator<}, should return \texttt{true} when \texttt{lhs} is considered to come before \texttt{rhs} in the ordering. To use \texttt{sort} here, we pass a third argument containing our custom comparison: \texttt{sort(array, array + n, cmp)}. Passing a custom comparison function to a standard library data structure is possible but unfortunately quite confusing and will not be covered here. This can be done more easily using the final method.

The final method uses a functor, which is an object that can also work as a function. This is done by implementing the function \texttt{operator()()}. In the case of sorting, the functor works somewhat like a Java \texttt{Comparator}.

\begin{mylstlisting}[language=C++]
struct cmp {
	bool operator()(const std::pair<int, int>& lhs, const std::pair<int, int>& rhs) {
		return lhs.x < rhs.x || lhs.x == rhs.x && lhs.y > rhs.y;
	}
};
\end{mylstlisting}

The functor method works just as the custom comparison function for \texttt{sort}: \texttt{sort(array, array + n, cmp)}. However, since it is now an object, it can be passed as a template argument to standard library containers much more easily than the comparison function can. For example, for the binary search tree implemented by C++, we can pass a custom comparison using \texttt{set<int, cmp> s}.

\section{Binary Search}

Before we begin studying data structures in Chapter \ref{chap:standard_ds}, it is necessary to first understand some search techniques. Suppose we're given a list and we want to check if it contains some element. We can do this directly by looking at every element of the list in a \emph{linear search}.

One way we can make this faster is by imposing an ordering on the list. Then inserting elements becomes slower because we can no longer just pop the element to the the end of the list. However, if we are asked to find an element, we know roughly where to search because we can compare our element to other elements in the list. In fact, we only need to look at $O(\log n)$ elements to check if the queried element exists.

Consider a sorted array that supports $O(1)$ access to any element. We can compare our queried element to the middle element of the array. Depending on whether our element is larger, smaller, or equal to the middle element, we'll be able to eliminate at least half of the array. Thus finding an element in a sorted array with \emph{binary search} is an $O(\log{n})$ operation. We can implement this either iteratively or recursively.

